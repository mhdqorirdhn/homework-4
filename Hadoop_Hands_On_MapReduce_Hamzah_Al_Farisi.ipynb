{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIsVT46Je21m"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "Hadoop is an open-source framework which is mainly used for storage purposes and maintaining and analyzing a large amount of data or datasets on the clusters of commodity hardware, which means it is actually a data management tool.\n",
        "\n",
        "##Hadoop mainly works on 3 different modes:\n",
        "\n",
        "###Standalone Mode\n",
        "\n",
        "Pseudo-distributed Mode\n",
        "Fully-distributed Mode\n",
        "Standalone Mode\n",
        "\n",
        "By default, Hadoop is configured to run in a non distributed mode. It runs as a single Java process. Instead of HDFS, this mode utilizes the local file system. This mode is useful for debugging and there isn't any need to configure core-site.xml, hdfs-site.xml, mapred-site.xml, masters & slaves. Stand-alone mode is usually the fastest mode in Hadoop.\n",
        "\n",
        "###Pseudo-distributed Mode\n",
        "\n",
        "Hadoop can also run on a single node in a Pseudo-distributed mode. In this mode, each daemon runs on separate java processes. In this mode custom configuration is required (core-site.xml, hdfs-site.xml, mapred-site.xml). Here HDFS is utilized for input and output. This mode of deployment is useful for testing and debugging purposes.\n",
        "\n",
        "###Fully-distributed Mode\n",
        "\n",
        "This is the production mode of Hadoop. In this mode typically one machine in the cluster is designated as NameNode and another as Resource Manager exclusively. These are masters. All other nodes act as Data Node and Node Manager. These are the slaves. Configuration parameters and environment need to be specified for Hadoop Daemons.\n",
        "\n",
        "Installing Java 8\n",
        "Hadoop is a java programming-based data processing framework\n",
        "\n",
        "OpenJDK is a development environment for building applications, applets, and components using the Java programming language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SypF2Xj6OvmD"
      },
      "source": [
        "# Installing Java 8\n",
        "Hadoop is a java programming-based data processing framework\n",
        "\n",
        "OpenJDK is a development environment for building applications, applets, and components using the Java programming language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lqfPfV3BIxlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe7cb97-7d00-4bb5-b9c3-789fa924eef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.24\" 2024-07-16\n",
            "OpenJDK Runtime Environment (build 11.0.24+8-post-Ubuntu-1ubuntu322.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.24+8-post-Ubuntu-1ubuntu322.04, mixed mode, sharing)\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n",
            "openjdk version \"1.8.0_422\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_422-8u422-b05-1~22.04-b05)\n",
            "OpenJDK 64-Bit Server VM (build 25.422-b05, mixed mode)\n",
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n",
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n",
            "#Port 22\n",
            "#GatewayPorts no\n",
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:Gajr+RdtJYLFyL6SpKQ7AEVzu4NQ+x3tkn4g2/oM6s4 root@57ee5aae6485\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "| .+ .. o         |\n",
            "| ..+ .ooo        |\n",
            "|... ..ooo        |\n",
            "|...o.+o+.o. .    |\n",
            "|.o.o*.=.So o     |\n",
            "|o . oB.o. o      |\n",
            "|..  +.o .o       |\n",
            "|o. o = ..        |\n",
            "| +E +o+.         |\n",
            "+----[SHA256]-----+\n",
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDMG5x3W1GtjkVtL7uOLfM6pTDzk8hk4KLrpC2H+ndXTZiNoG+vMb0zP1IGrbTx\n",
            "AorZvvkX6lz2iqoDGfr80bHRsLSyN3qUogbp6UzHzjJs1oQXoJgMDdV1BWU5nyD0hgjbzjJdH2QR95FS5yRKzNFO2u6AVqXHn/1O\n",
            "3RyR32LrxRoacFS75yRMZrH38QBbxkz1DRffBIHg+ErOdeorETNh6Cl6/3ZirswiO7DkTW0siDbrM4R7fwgzH6xYzeHQp1KQ0Ts1\n",
            "tQjep/pOHBGdiwEHeVVdIfEc9Ls4/Bs/qZxssDrT8xVJsKMVGiqgwjcw8eL2M7X9zedkYvUssQHD7XQ78sjSpo4jIlzilyzFuSi/\n",
            "Zs6q9jHrgIliT5yUdoNY6tAB0N3oePAH9IHY+GWtVJw/etQjBY1FCwE9YgwEIy5Nm8j9o3Aka+5jNtZ7j1PPxOTVrPe78OoiQ+hF\n",
            "azBSWJ7+tBplGDfKEMNVZc479hxDBom/lKPD0MPYoXlMoqMpm/0= root@57ee5aae6485\n",
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
            " 06:03:35 up 1 min,  0 users,  load average: 2.67, 0.90, 0.33\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!java -version\n",
        "\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac\n",
        "!update-alternatives --set jps /usr/lib/jvm/java-8-openjdk-amd64/bin/jps\n",
        "!java -version\n",
        "\n",
        "#Finding the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "!apt-get install openssh-server -qq > /dev/null\n",
        "!service ssh start\n",
        "\n",
        "!grep Port /etc/ssh/sshd_config\n",
        "\n",
        "#Creating a new rsa key pair with empty password\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa <<< y\n",
        "\n",
        "# See id_rsa.pub content\n",
        "!more /root/.ssh/id_rsa.pub\n",
        "\n",
        "#Copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub > $HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys\n",
        "\n",
        "#Conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime\n",
        "\n",
        "\n",
        "#Downloading Hadoop 3.2.3\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
        "\n",
        "#Untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "#Removing the tar file\n",
        "!rm hadoop-3.2.3.tar.gz\n",
        "\n",
        "\n",
        "#Copying the hadoop files to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/\n",
        "#-r copy directories recursively\n",
        "\n",
        "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh\n",
        "\n",
        "import os\n",
        "#Creating environment variables\n",
        "#Creating Hadoop home variable\n",
        "\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += f'{os.environ[\"JAVA_HOME\"]}/bin:{os.environ[\"JRE_HOME\"]}/bin:{os.environ[\"HADOOP_HOME\"]}/sbin'\n",
        "\n",
        "#Dowloading text example to use as input\n",
        "# !wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK_Ozh8RVza-"
      },
      "outputs": [],
      "source": [
        "#Adding required property to core-site.xlm file\n",
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/core-site.xml\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "\n",
        "<!-- Put site-specific property overrides in this file. -->\n",
        "\n",
        "<configuration>\n",
        "  <property>\n",
        "          <name>fs.defaultFS</name>\n",
        "          <value>hdfs://localhost:9000</value>\n",
        "          <description>Where HDFS NameNode can be found on the network</description>\n",
        "  </property>\n",
        "</configuration>\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRlijZzZ5_IQ"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "\n",
        "<!-- Put site-specific property overrides in this file. -->\n",
        "\n",
        "<configuration>\n",
        "<property>\n",
        "    <name>dfs.replication</name>\n",
        "    <value>1</value>\n",
        "  </property>\n",
        "\n",
        "</configuration>\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI7EXvcy8x5Q"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
        "<?xml version=\"1.0\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "\n",
        "<!-- Put site-specific property overrides in this file. -->\n",
        "\n",
        "<configuration>\n",
        "<property>\n",
        "    <name>mapreduce.framework.name</name>\n",
        "    <value>yarn</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>mapreduce.application.classpath</name>\n",
        "    <value>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>\n",
        "  </property>\n",
        "\n",
        "</configuration>\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXqsVh1M9Pxu"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
        "<?xml version=\"1.0\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "<configuration>\n",
        "<property>\n",
        "    <description>The hostname of the RM.</description>\n",
        "    <name>yarn.resourcemanager.hostname</name>\n",
        "    <value>localhost</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>yarn.nodemanager.aux-services</name>\n",
        "    <value>mapreduce_shuffle</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>yarn.nodemanager.env-whitelist</name>\n",
        "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\n",
        "  </property>\n",
        "\n",
        "<!-- Site specific YARN configuration properties -->\n",
        "\n",
        "</configuration>\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeDr2ydyXYFK"
      },
      "source": [
        "# Formatting the HDFS Filesystem\n",
        "\n",
        "Before HDFS can be used for the first time the file system must be formatted. The formatting process creates an empty file system by creating the storage directories and the initial versions of the NameNodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmrjTfnQ9rzL",
        "outputId": "cfaa2eaf-a58f-4a61-9d3b-3c9fac015765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.2.3/logs does not exist. Creating.\n",
            "2024-08-17 06:04:25,012 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 57ee5aae6485/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.3\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.2.3/etc/hadoop:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z\n",
            "STARTUP_MSG:   java = 1.8.0_422\n",
            "************************************************************/\n",
            "2024-08-17 06:04:25,060 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2024-08-17 06:04:25,232 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-0adb6050-cd30-4a3e-91c4-0905b0483f01\n",
            "2024-08-17 06:04:25,981 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2024-08-17 06:04:26,020 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2024-08-17 06:04:26,022 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2024-08-17 06:04:26,023 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2024-08-17 06:04:26,031 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2024-08-17 06:04:26,031 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2024-08-17 06:04:26,032 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2024-08-17 06:04:26,032 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2024-08-17 06:04:26,092 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2024-08-17 06:04:26,103 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2024-08-17 06:04:26,103 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2024-08-17 06:04:26,108 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2024-08-17 06:04:26,108 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Aug 17 06:04:26\n",
            "2024-08-17 06:04:26,110 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2024-08-17 06:04:26,110 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-17 06:04:26,112 INFO util.GSet: 2.0% max memory 2.8 GB = 57.7 MB\n",
            "2024-08-17 06:04:26,112 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2024-08-17 06:04:26,146 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2024-08-17 06:04:26,146 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2024-08-17 06:04:26,155 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2024-08-17 06:04:26,155 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2024-08-17 06:04:26,155 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2024-08-17 06:04:26,156 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
            "2024-08-17 06:04:26,156 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2024-08-17 06:04:26,160 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2024-08-17 06:04:26,160 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2024-08-17 06:04:26,161 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2024-08-17 06:04:26,161 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2024-08-17 06:04:26,161 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2024-08-17 06:04:26,201 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2024-08-17 06:04:26,201 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2024-08-17 06:04:26,201 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2024-08-17 06:04:26,201 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2024-08-17 06:04:26,216 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2024-08-17 06:04:26,216 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-17 06:04:26,217 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2024-08-17 06:04:26,217 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2024-08-17 06:04:26,219 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2024-08-17 06:04:26,219 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2024-08-17 06:04:26,220 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2024-08-17 06:04:26,220 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2024-08-17 06:04:26,225 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2024-08-17 06:04:26,227 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2024-08-17 06:04:26,232 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2024-08-17 06:04:26,232 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-17 06:04:26,232 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2024-08-17 06:04:26,232 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2024-08-17 06:04:26,241 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2024-08-17 06:04:26,241 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2024-08-17 06:04:26,241 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2024-08-17 06:04:26,246 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2024-08-17 06:04:26,246 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2024-08-17 06:04:26,248 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2024-08-17 06:04:26,248 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-17 06:04:26,248 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 886.4 KB\n",
            "2024-08-17 06:04:26,248 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2024-08-17 06:04:26,281 INFO namenode.FSImage: Allocated new BlockPoolId: BP-107238666-172.28.0.12-1723874666270\n",
            "2024-08-17 06:04:26,305 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2024-08-17 06:04:26,349 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2024-08-17 06:04:26,472 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2024-08-17 06:04:26,492 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2024-08-17 06:04:26,534 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2024-08-17 06:04:26,535 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2024-08-17 06:04:26,540 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2024-08-17 06:04:26,541 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 57ee5aae6485/172.28.0.12\n",
            "************************************************************/\n",
            "Starting namenodes on [57ee5aae6485]\n",
            "57ee5aae6485: Warning: Permanently added '57ee5aae6485' (ED25519) to the list of known hosts.\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [57ee5aae6485]\n",
            "nohup: ignoring input and appending output to 'nohup.out'\n",
            "1975 Jps\n",
            "1870 NodeManager\n",
            "1759 ResourceManager\n"
          ]
        }
      ],
      "source": [
        "!$HADOOP_HOME/bin/hdfs namenode -format\n",
        "\n",
        "#Creating other necessary enviroment variables before starting nodes\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\"\n",
        "\n",
        "#Launching hdfs deamons\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh\n",
        "\n",
        "#Launching yarn deamons\n",
        "#nohup causes a process to ignore a SIGHUP signal\n",
        "!nohup $HADOOP_HOME/sbin/start-yarn.sh\n",
        "\n",
        "#Listing the running deamons\n",
        "!jps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srV27xSGXu5O"
      },
      "source": [
        "### Monitoring Hadoop cluster with hadoop admin commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUxnt00v_wqC",
        "outputId": "951d1cbd-c0a0-446b-ab4e-32a455967c46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "report: FileSystem file:/// is not an HDFS file system. The fs class is: org.apache.hadoop.fs.LocalFileSystem\n",
            "Usage: hdfs dfsadmin [-report] [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]\n"
          ]
        }
      ],
      "source": [
        "#Report the basic file system information and statistics\n",
        "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkINc9skYBld"
      },
      "source": [
        "# MapReduce"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True: pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "Db1-dQ_gAlA0",
        "outputId": "ea4efb8f-83e3-421e-b5f3-88736ee12c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b16dc615ea65>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input (if it has not been donwloaded yet)\n",
        "# !wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt\n",
        "\n",
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count\n",
        "#Coping file from local file system to HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/pembukaan_uud1945.txt /word_count\n",
        "\n",
        "#Exploring Hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count\n",
        "\n",
        "# Run MapReduce Example using JAVA\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount /word_count/pembukaan_uud1945.txt /word_count/output/"
      ],
      "metadata": {
        "id": "-VjzSc7jRrOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4eb4e2-23e4-49d2-9df6-af5ff1d0f2ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root root       1504 2024-08-17 06:05 /word_count/pembukaan_uud1945.txt\n",
            "2024-08-17 06:05:39,634 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-08-17 06:05:39,732 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-08-17 06:05:39,732 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-08-17 06:05:39,943 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2024-08-17 06:05:39,984 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-08-17 06:05:40,163 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local40864422_0001\n",
            "2024-08-17 06:05:40,163 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-08-17 06:05:40,361 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-08-17 06:05:40,362 INFO mapreduce.Job: Running job: job_local40864422_0001\n",
            "2024-08-17 06:05:40,369 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-08-17 06:05:40,377 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-17 06:05:40,377 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-17 06:05:40,378 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2024-08-17 06:05:40,416 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-08-17 06:05:40,417 INFO mapred.LocalJobRunner: Starting task: attempt_local40864422_0001_m_000000_0\n",
            "2024-08-17 06:05:40,443 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-17 06:05:40,443 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-17 06:05:40,468 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-17 06:05:40,473 INFO mapred.MapTask: Processing split: file:/word_count/pembukaan_uud1945.txt:0+1504\n",
            "2024-08-17 06:05:40,576 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-08-17 06:05:40,576 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-08-17 06:05:40,576 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-08-17 06:05:40,576 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-08-17 06:05:40,576 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-08-17 06:05:40,582 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-08-17 06:05:40,599 INFO mapred.LocalJobRunner: \n",
            "2024-08-17 06:05:40,600 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-08-17 06:05:40,600 INFO mapred.MapTask: Spilling map output\n",
            "2024-08-17 06:05:40,600 INFO mapred.MapTask: bufstart = 0; bufend = 2285; bufvoid = 104857600\n",
            "2024-08-17 06:05:40,600 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213616(104854464); length = 781/6553600\n",
            "2024-08-17 06:05:40,622 INFO mapred.MapTask: Finished spill 0\n",
            "2024-08-17 06:05:40,636 INFO mapred.Task: Task:attempt_local40864422_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-08-17 06:05:40,644 INFO mapred.LocalJobRunner: map\n",
            "2024-08-17 06:05:40,644 INFO mapred.Task: Task 'attempt_local40864422_0001_m_000000_0' done.\n",
            "2024-08-17 06:05:40,653 INFO mapred.Task: Final Counters for attempt_local40864422_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318157\n",
            "\t\tFILE: Number of bytes written=860062\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=11\n",
            "\t\tMap output records=196\n",
            "\t\tMap output bytes=2285\n",
            "\t\tMap output materialized bytes=1960\n",
            "\t\tInput split bytes=103\n",
            "\t\tCombine input records=196\n",
            "\t\tCombine output records=138\n",
            "\t\tSpilled Records=138\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1532\n",
            "2024-08-17 06:05:40,653 INFO mapred.LocalJobRunner: Finishing task: attempt_local40864422_0001_m_000000_0\n",
            "2024-08-17 06:05:40,654 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-08-17 06:05:40,657 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-08-17 06:05:40,658 INFO mapred.LocalJobRunner: Starting task: attempt_local40864422_0001_r_000000_0\n",
            "2024-08-17 06:05:40,664 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-17 06:05:40,665 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-17 06:05:40,665 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-17 06:05:40,668 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@a25b9f5\n",
            "2024-08-17 06:05:40,670 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-17 06:05:40,691 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-08-17 06:05:40,697 INFO reduce.EventFetcher: attempt_local40864422_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-08-17 06:05:40,750 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local40864422_0001_m_000000_0 decomp: 1956 len: 1960 to MEMORY\n",
            "2024-08-17 06:05:40,754 INFO reduce.InMemoryMapOutput: Read 1956 bytes from map-output for attempt_local40864422_0001_m_000000_0\n",
            "2024-08-17 06:05:40,756 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1956, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1956\n",
            "2024-08-17 06:05:40,757 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-08-17 06:05:40,758 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-17 06:05:40,759 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-08-17 06:05:40,769 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-17 06:05:40,769 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1952 bytes\n",
            "2024-08-17 06:05:40,778 INFO reduce.MergeManagerImpl: Merged 1 segments, 1956 bytes to disk to satisfy reduce memory limit\n",
            "2024-08-17 06:05:40,778 INFO reduce.MergeManagerImpl: Merging 1 files, 1960 bytes from disk\n",
            "2024-08-17 06:05:40,779 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-08-17 06:05:40,779 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-17 06:05:40,783 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1952 bytes\n",
            "2024-08-17 06:05:40,785 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-17 06:05:40,790 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-08-17 06:05:40,802 INFO mapred.Task: Task:attempt_local40864422_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-08-17 06:05:40,804 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-17 06:05:40,805 INFO mapred.Task: Task attempt_local40864422_0001_r_000000_0 is allowed to commit now\n",
            "2024-08-17 06:05:40,810 INFO output.FileOutputCommitter: Saved output of task 'attempt_local40864422_0001_r_000000_0' to file:/word_count/output\n",
            "2024-08-17 06:05:40,812 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-08-17 06:05:40,813 INFO mapred.Task: Task 'attempt_local40864422_0001_r_000000_0' done.\n",
            "2024-08-17 06:05:40,813 INFO mapred.Task: Final Counters for attempt_local40864422_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=322109\n",
            "\t\tFILE: Number of bytes written=863445\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=138\n",
            "\t\tReduce shuffle bytes=1960\n",
            "\t\tReduce input records=138\n",
            "\t\tReduce output records=138\n",
            "\t\tSpilled Records=138\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=13\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1423\n",
            "2024-08-17 06:05:40,813 INFO mapred.LocalJobRunner: Finishing task: attempt_local40864422_0001_r_000000_0\n",
            "2024-08-17 06:05:40,813 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-08-17 06:05:41,368 INFO mapreduce.Job: Job job_local40864422_0001 running in uber mode : false\n",
            "2024-08-17 06:05:41,369 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-08-17 06:05:41,370 INFO mapreduce.Job: Job job_local40864422_0001 completed successfully\n",
            "2024-08-17 06:05:41,378 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=640266\n",
            "\t\tFILE: Number of bytes written=1723507\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=11\n",
            "\t\tMap output records=196\n",
            "\t\tMap output bytes=2285\n",
            "\t\tMap output materialized bytes=1960\n",
            "\t\tInput split bytes=103\n",
            "\t\tCombine input records=196\n",
            "\t\tCombine output records=138\n",
            "\t\tReduce input groups=138\n",
            "\t\tReduce shuffle bytes=1960\n",
            "\t\tReduce input records=138\n",
            "\t\tReduce output records=138\n",
            "\t\tSpilled Records=276\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=13\n",
            "\t\tTotal committed heap usage (bytes)=409993216\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1532\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output"
      ],
      "metadata": {
        "id": "k6_U5EszSUy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ed6ded-d04d-4408-929b-87206f138a06"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-08-17 06:05 /word_count/output/_SUCCESS\n",
            "-rw-r--r--   1 root root       1403 2024-08-17 06:05 /word_count/output/part-r-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output/part-r-00000 | head -50"
      ],
      "metadata": {
        "id": "tmhHNUEeSbyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d9c603-42aa-43d3-9c23-cb51b3d99872"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\t1\n",
            "1945\t1\n",
            "Allah\t1\n",
            "Atas\t1\n",
            "Bahwa\t1\n",
            "DASAR\t1\n",
            "Dan\t1\n",
            "Dasar\t1\n",
            "Esa,\t1\n",
            "INDONESIA\t1\n",
            "Indonesia\t9\n",
            "Indonesia,\t2\n",
            "Indonesia.\t1\n",
            "Keadilan\t1\n",
            "Kebangsaan\t1\n",
            "Kemanusiaan\t1\n",
            "Kemerdekaan\t2\n",
            "Kemudian\t1\n",
            "Kerakyatan\t1\n",
            "Ketuhanan\t1\n",
            "Kuasa\t1\n",
            "Maha\t2\n",
            "NEGARA\t1\n",
            "Negara\t4\n",
            "P\t1\n",
            "PEMBUKAAN\t1\n",
            "Pemerintah\t1\n",
            "Permusyawaratan/Perwakilan,\t1\n",
            "Persatuan\t1\n",
            "REPUBLIK\t1\n",
            "Republik\t1\n",
            "TAHUN\t1\n",
            "UNDANG-UNDANG\t1\n",
            "Undang-Undang\t1\n",
            "Yang\t2\n",
            "a\t1\n",
            "abadi\t1\n",
            "adil\t2\n",
            "atas\t1\n",
            "b\t1\n",
            "bagi\t1\n",
            "bangsa\t2\n",
            "bangsa,\t1\n",
            "bebas,\t1\n",
            "beradab,\t1\n",
            "berbahagia\t1\n",
            "berdasar\t1\n",
            "berdasarkan\t1\n",
            "berdaulat,\t1\n",
            "berkat\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hadoop Streaming Using Python\n",
        "\n",
        "Hadoop Streaming is a feature that comes with Hadoop and allows users or developers to use various different languages for writing MapReduce programs like Python, C++, Ruby, etc.\n",
        "\n",
        "The utility will create a Map/Reduce job, submit the job to an appropriate cluster, and monitor the progress of the job until it completes."
      ],
      "metadata": {
        "id": "2Cq8XtDRTBHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Hadoop utilities available\n",
        "!ls $HADOOP_HOME/share/hadoop/tools/lib/"
      ],
      "metadata": {
        "id": "k_Dk8vLYTDqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e249a8-67d3-438a-83e3-37fc7739848e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aliyun-java-sdk-core-4.5.10.jar      hadoop-gridmix-3.2.3.jar\n",
            "aliyun-java-sdk-kms-2.11.0.jar\t     hadoop-kafka-3.2.3.jar\n",
            "aliyun-java-sdk-ram-3.1.0.jar\t     hadoop-openstack-3.2.3.jar\n",
            "aliyun-sdk-oss-3.13.0.jar\t     hadoop-resourceestimator-3.2.3.jar\n",
            "aws-java-sdk-bundle-1.11.901.jar     hadoop-rumen-3.2.3.jar\n",
            "azure-data-lake-store-sdk-2.2.9.jar  hadoop-sls-3.2.3.jar\n",
            "azure-keyvault-core-1.0.0.jar\t     hadoop-streaming-3.2.3.jar\n",
            "azure-storage-7.0.0.jar\t\t     ini4j-0.5.4.jar\n",
            "hadoop-aliyun-3.2.3.jar\t\t     jdom2-2.0.6.jar\n",
            "hadoop-archive-logs-3.2.3.jar\t     kafka-clients-2.8.1.jar\n",
            "hadoop-archives-3.2.3.jar\t     lz4-java-1.7.1.jar\n",
            "hadoop-aws-3.2.3.jar\t\t     ojalgo-43.0.jar\n",
            "hadoop-azure-3.2.3.jar\t\t     opentracing-api-0.33.0.jar\n",
            "hadoop-azure-datalake-3.2.3.jar      opentracing-noop-0.33.0.jar\n",
            "hadoop-datajoin-3.2.3.jar\t     opentracing-util-0.33.0.jar\n",
            "hadoop-distcp-3.2.3.jar\t\t     org.jacoco.agent-0.8.5-runtime.jar\n",
            "hadoop-extras-3.2.3.jar\t\t     wildfly-openssl-1.0.7.Final.jar\n",
            "hadoop-fs2img-3.2.3.jar\t\t     zstd-jni-1.4.9-1.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input (if it has not been donwloaded yet)\n",
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "id": "Nkl5QNWGHmX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count_with_python"
      ],
      "metadata": {
        "id": "f--LlKTSTHDe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/pembukaan_uud1945.txt /word_count_with_python"
      ],
      "metadata": {
        "id": "JjGgo6_PTMe5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapper\n",
        "\n",
        "The mapper is an executable that reads all input records from a file/s and generates an output in the form of key-value pairs which works as input for the Reducer."
      ],
      "metadata": {
        "id": "DNYaZK6WTQgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "\n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "  # to remove leading and trailing whitespace\n",
        "  ###\n",
        "  ### \"sadsadas dsdasda\"\n",
        "  ### sadsadas\n",
        "  ### sadsadas\n",
        "  line = line.strip()\n",
        "  # split the line into words, output data type list\n",
        "  words = line.split()\n",
        "\n",
        "  # we are looping over the words array and printing the word\n",
        "  # with the count of 1 to the STDOUT\n",
        "  for word in words:\n",
        "    # write the results to STDOUT (standard output);\n",
        "    # what we output here will be the input for the\n",
        "    # Reduce step, i.e. the input for reducer.py\n",
        "    print('%s\\t%s' % (word, 1))"
      ],
      "metadata": {
        "id": "EJrx3ZPoTOyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a489e4d2-c48f-4213-8ca0-c47cac6ade1c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line = \"Hi, I'm Bruce Sterling, the author of this electronic book. \"\n",
        "\n",
        "line = line.strip()\n",
        "words = line.split()\n",
        "\n",
        "for word in words:\n",
        "  data = '%s\\t%s' % (word, 1)\n",
        "\n",
        "  word, count = data.split('\\t', 1)\n",
        "  print(word, count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffbLTCcNEcF9",
        "outputId": "7a64fd83-b8eb-468e-a949-ab2d1e986d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi, 1\n",
            "I'm 1\n",
            "Bruce 1\n",
            "Sterling, 1\n",
            "the 1\n",
            "author 1\n",
            "of 1\n",
            "this 1\n",
            "electronic 1\n",
            "book. 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducer\n",
        "\n",
        "The reducer is an executable that reads all the intermediate key-value pairs generated by the mapper and generates a final output as a result of a computation operation like addition, filtration, and aggregation.\n",
        "\n",
        "Both the mapper and the reducer read the input from stdin (line by line) and emit the output to stdout."
      ],
      "metadata": {
        "id": "BBlWq4AaTWMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "  # remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # splitting the data on the basis of tab we have provided in mapper.py\n",
        "  word, count = line.split('\\t', 1)\n",
        "  # convert count (currently a string) to int\n",
        "  try:\n",
        "    count = int(count)\n",
        "  except ValueError:\n",
        "    # count was not a number, so silently\n",
        "    # ignore/discard this line\n",
        "    continue\n",
        "\n",
        "  # this IF-switch only works because Hadoop sorts map output\n",
        "  # by key (here: word) before it is passed to the reducer\n",
        "  if current_word == word:\n",
        "    current_count += count\n",
        "  else:\n",
        "    if current_word: #to not print current_word=None\n",
        "      # write result to STDOUT\n",
        "      print('%s\\t%s' % (current_word, current_count))\n",
        "    current_count = count\n",
        "    current_word = word\n",
        "\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "  print('%s\\t%s' % (current_word, current_count))"
      ],
      "metadata": {
        "id": "WKWl7oZOTV2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230a54ea-505c-40b7-cbc6-f4658d84540f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat pembukaan_uud1945.txt | python mapper.py | sort -k1,1 | python reducer.py | head -10\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "id": "DgnoB_WaTfrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70958de1-96ca-4447-8902-aa9060ee7aa3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\t1\n",
            "1945\t1\n",
            "a\t1\n",
            "abadi\t1\n",
            "adil\t2\n",
            "Allah\t1\n",
            "atas\t1\n",
            "Atas\t1\n",
            "b\t1\n",
            "bagi\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper.py /content/reducer.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users"
      ],
      "metadata": {
        "id": "t5k-wHESTgc3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input /word_count_with_python/pembukaan_uud1945.txt \\\n",
        "  -output /word_count_with_python/output \\\n",
        "  -mapper \"python /content/mapper.py\" \\\n",
        "  -reducer \"python /content/reducer.py\""
      ],
      "metadata": {
        "id": "PHuUF1kTTj-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ffc8e3-25e0-4933-b5df-5f92000f4124"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-17 06:07:37,089 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-08-17 06:07:37,193 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-08-17 06:07:37,193 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-08-17 06:07:37,209 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-17 06:07:37,422 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-08-17 06:07:37,447 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-08-17 06:07:37,637 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1640320223_0001\n",
            "2024-08-17 06:07:37,637 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-08-17 06:07:37,836 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-08-17 06:07:37,838 INFO mapreduce.Job: Running job: job_local1640320223_0001\n",
            "2024-08-17 06:07:37,845 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-08-17 06:07:37,846 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-08-17 06:07:37,851 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-17 06:07:37,851 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-17 06:07:37,890 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-08-17 06:07:37,894 INFO mapred.LocalJobRunner: Starting task: attempt_local1640320223_0001_m_000000_0\n",
            "2024-08-17 06:07:37,923 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-17 06:07:37,923 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-17 06:07:37,950 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-17 06:07:37,960 INFO mapred.MapTask: Processing split: file:/word_count_with_python/pembukaan_uud1945.txt:0+1504\n",
            "2024-08-17 06:07:37,972 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-08-17 06:07:38,054 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-08-17 06:07:38,054 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-08-17 06:07:38,054 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-08-17 06:07:38,054 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-08-17 06:07:38,054 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-08-17 06:07:38,057 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-08-17 06:07:38,065 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/mapper.py]\n",
            "2024-08-17 06:07:38,072 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-08-17 06:07:38,073 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-08-17 06:07:38,073 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-08-17 06:07:38,073 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-08-17 06:07:38,074 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-08-17 06:07:38,074 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-08-17 06:07:38,074 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-08-17 06:07:38,075 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-08-17 06:07:38,075 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-08-17 06:07:38,076 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-08-17 06:07:38,076 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-08-17 06:07:38,077 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-08-17 06:07:38,111 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-17 06:07:38,111 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-17 06:07:38,173 INFO streaming.PipeMapRed: Records R/W=11/1\n",
            "2024-08-17 06:07:38,176 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-08-17 06:07:38,179 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-08-17 06:07:38,183 INFO mapred.LocalJobRunner: \n",
            "2024-08-17 06:07:38,183 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-08-17 06:07:38,183 INFO mapred.MapTask: Spilling map output\n",
            "2024-08-17 06:07:38,183 INFO mapred.MapTask: bufstart = 0; bufend = 1893; bufvoid = 104857600\n",
            "2024-08-17 06:07:38,183 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213616(104854464); length = 781/6553600\n",
            "2024-08-17 06:07:38,196 INFO mapred.MapTask: Finished spill 0\n",
            "2024-08-17 06:07:38,210 INFO mapred.Task: Task:attempt_local1640320223_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-08-17 06:07:38,213 INFO mapred.LocalJobRunner: Records R/W=11/1\n",
            "2024-08-17 06:07:38,213 INFO mapred.Task: Task 'attempt_local1640320223_0001_m_000000_0' done.\n",
            "2024-08-17 06:07:38,221 INFO mapred.Task: Final Counters for attempt_local1640320223_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=178223\n",
            "\t\tFILE: Number of bytes written=729506\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=11\n",
            "\t\tMap output records=196\n",
            "\t\tMap output bytes=1893\n",
            "\t\tMap output materialized bytes=2291\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=196\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1532\n",
            "2024-08-17 06:07:38,222 INFO mapred.LocalJobRunner: Finishing task: attempt_local1640320223_0001_m_000000_0\n",
            "2024-08-17 06:07:38,222 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-08-17 06:07:38,226 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-08-17 06:07:38,226 INFO mapred.LocalJobRunner: Starting task: attempt_local1640320223_0001_r_000000_0\n",
            "2024-08-17 06:07:38,235 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-17 06:07:38,235 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-17 06:07:38,236 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-17 06:07:38,254 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@57795756\n",
            "2024-08-17 06:07:38,256 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-17 06:07:38,284 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-08-17 06:07:38,290 INFO reduce.EventFetcher: attempt_local1640320223_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-08-17 06:07:38,326 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1640320223_0001_m_000000_0 decomp: 2287 len: 2291 to MEMORY\n",
            "2024-08-17 06:07:38,329 INFO reduce.InMemoryMapOutput: Read 2287 bytes from map-output for attempt_local1640320223_0001_m_000000_0\n",
            "2024-08-17 06:07:38,331 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2287, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2287\n",
            "2024-08-17 06:07:38,332 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-08-17 06:07:38,333 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-17 06:07:38,333 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-08-17 06:07:38,342 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-17 06:07:38,343 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2283 bytes\n",
            "2024-08-17 06:07:38,348 INFO reduce.MergeManagerImpl: Merged 1 segments, 2287 bytes to disk to satisfy reduce memory limit\n",
            "2024-08-17 06:07:38,351 INFO reduce.MergeManagerImpl: Merging 1 files, 2291 bytes from disk\n",
            "2024-08-17 06:07:38,352 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-08-17 06:07:38,352 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-17 06:07:38,355 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2283 bytes\n",
            "2024-08-17 06:07:38,355 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-17 06:07:38,368 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/reducer.py]\n",
            "2024-08-17 06:07:38,374 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-08-17 06:07:38,376 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-08-17 06:07:38,414 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-17 06:07:38,415 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-17 06:07:38,417 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-17 06:07:38,482 INFO streaming.PipeMapRed: Records R/W=196/1\n",
            "2024-08-17 06:07:38,488 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-08-17 06:07:38,489 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-08-17 06:07:38,489 INFO mapred.Task: Task:attempt_local1640320223_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-08-17 06:07:38,491 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-17 06:07:38,491 INFO mapred.Task: Task attempt_local1640320223_0001_r_000000_0 is allowed to commit now\n",
            "2024-08-17 06:07:38,494 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1640320223_0001_r_000000_0' to file:/word_count_with_python/output\n",
            "2024-08-17 06:07:38,497 INFO mapred.LocalJobRunner: Records R/W=196/1 > reduce\n",
            "2024-08-17 06:07:38,497 INFO mapred.Task: Task 'attempt_local1640320223_0001_r_000000_0' done.\n",
            "2024-08-17 06:07:38,497 INFO mapred.Task: Final Counters for attempt_local1640320223_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=182837\n",
            "\t\tFILE: Number of bytes written=733220\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=138\n",
            "\t\tReduce shuffle bytes=2291\n",
            "\t\tReduce input records=196\n",
            "\t\tReduce output records=138\n",
            "\t\tSpilled Records=196\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1423\n",
            "2024-08-17 06:07:38,498 INFO mapred.LocalJobRunner: Finishing task: attempt_local1640320223_0001_r_000000_0\n",
            "2024-08-17 06:07:38,498 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-08-17 06:07:38,843 INFO mapreduce.Job: Job job_local1640320223_0001 running in uber mode : false\n",
            "2024-08-17 06:07:38,844 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-08-17 06:07:38,845 INFO mapreduce.Job: Job job_local1640320223_0001 completed successfully\n",
            "2024-08-17 06:07:38,854 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=361060\n",
            "\t\tFILE: Number of bytes written=1462726\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=11\n",
            "\t\tMap output records=196\n",
            "\t\tMap output bytes=1893\n",
            "\t\tMap output materialized bytes=2291\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=138\n",
            "\t\tReduce shuffle bytes=2291\n",
            "\t\tReduce input records=196\n",
            "\t\tReduce output records=138\n",
            "\t\tSpilled Records=392\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=409993216\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1532\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1423\n",
            "2024-08-17 06:07:38,855 INFO streaming.StreamJob: Output directory: /word_count_with_python/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count_with_python/output"
      ],
      "metadata": {
        "id": "C_nqizwZTmQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40441c9-1558-493f-ce0c-7e5ac16c4c10"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-08-17 06:07 /word_count_with_python/output/_SUCCESS\n",
            "-rw-r--r--   1 root root       1403 2024-08-17 06:07 /word_count_with_python/output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count_with_python/output/part-00000 | head -50"
      ],
      "metadata": {
        "id": "EKRPU4UKTogT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d57595-a2c2-41bb-9760-be293363c39f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\t1\n",
            "1945\t1\n",
            "Allah\t1\n",
            "Atas\t1\n",
            "Bahwa\t1\n",
            "DASAR\t1\n",
            "Dan\t1\n",
            "Dasar\t1\n",
            "Esa,\t1\n",
            "INDONESIA\t1\n",
            "Indonesia\t9\n",
            "Indonesia,\t2\n",
            "Indonesia.\t1\n",
            "Keadilan\t1\n",
            "Kebangsaan\t1\n",
            "Kemanusiaan\t1\n",
            "Kemerdekaan\t2\n",
            "Kemudian\t1\n",
            "Kerakyatan\t1\n",
            "Ketuhanan\t1\n",
            "Kuasa\t1\n",
            "Maha\t2\n",
            "NEGARA\t1\n",
            "Negara\t4\n",
            "P\t1\n",
            "PEMBUKAAN\t1\n",
            "Pemerintah\t1\n",
            "Permusyawaratan/Perwakilan,\t1\n",
            "Persatuan\t1\n",
            "REPUBLIK\t1\n",
            "Republik\t1\n",
            "TAHUN\t1\n",
            "UNDANG-UNDANG\t1\n",
            "Undang-Undang\t1\n",
            "Yang\t2\n",
            "a\t1\n",
            "abadi\t1\n",
            "adil\t2\n",
            "atas\t1\n",
            "b\t1\n",
            "bagi\t1\n",
            "bangsa\t2\n",
            "bangsa,\t1\n",
            "bebas,\t1\n",
            "beradab,\t1\n",
            "berbahagia\t1\n",
            "berdasar\t1\n",
            "berdasarkan\t1\n",
            "berdaulat,\t1\n",
            "berkat\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -copyToLocal /word_count_with_python/output/part-00000 /content/hdfs-wordcount.txt"
      ],
      "metadata": {
        "id": "7wrU43Da5rrW"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}